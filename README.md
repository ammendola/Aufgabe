# Datenintensive und datenfokussierte Aktivitäten in der ULB Münster

## Einleitung

In dieser Arbeit werden zunächst die wichtigsten datenbasierten Prozesse und Aktivitäten innerhalb der ULB Münster aufgelistet und darin jeweils kurz beschreiben, welche Daten in welcher Reihenfolge und in welcher Art und Weise in den Arbeitsprozessen fließen. Anschließend sollen anhand zweier Beispiele Optimierungspotentiale dieser Prozesse beleuchtet werden.

## Datenflüsse und -aktivitäten im ULB-Kerngeschäft

1. Online-Katalog + Discovery-System

Als eines der ältesten und wichtigsten datenbasierten Systeme ist der bereits in den 1990er entwickelte [Online-Katalog](https://katalogix.uni-muenster.de/Katalog/start.do) (damals noch OPAC) zu nennen. In diesem Katalog kann man alle Bücher, Zeitschriften und sonstigen Medien der ULB-Zentralbibliothek, der Zweigbibliotheken und aller Institutsbibliotheken der WWU Münster recherchieren. Als Sammelstelle dieser Katalogdaten dient eine zentrale Datenbank (= [PostgresSQL](https://www.postgresql.org/)), in welche alle benötigten Katalogdaten aus den kooperierenden Bibliotheken migriert werden; in den meisten Fällen über den für den bibliothekarischen Kontext entwickelten Standard MARC21XML. MARC21XML-Datenpakete werden per [FTP-Server](https://de.wikipedia.org/wiki/File_Transfer_Protocol) in den Index der zentralen Datenbank gespielt und im Rahmen dieses Datenaustausches so interpretiert und verstehbar gemacht (= Datenmapping), dass sie als brauchbare Katalogdaten im Online-Katalog angezeigt werden können. In die zentrale Datenbank gehen ebenso alle Katalogdaten aus dem Discovery-System der ULB ein (= [Primo Central der Fa. *Exlibris*](https://de.wikipedia.org/wiki/Primo_Central)). Diese Katalogdaten stammen allerdings aus dem Index des Discovery-Systems und werden per [OAI-Schnittstelle](http://www.openarchives.org/) in die zentrale Datenbank gespeist und verarbeitet. Sämtliche Daten aus der zentralen Datenbank werden zweifach abgesichert: Zum einen durch einen dreimal pro Woche automatisiert stattfindenden Backup auf einem externen Backup-Server, der den Katalogbestand auf dem Stand des zurückliegenden Monates absichert, zum anderen durch einen sog. Backup Slave, der als immerwährende Kopie der zentralen Datenbank (= Master) den aktuellen Zustand dupliziert und für den Fall eines plötzlichen (physischen) Verlustes der Datenbank gedacht ist. Im Hinblick auf den Online-Katalog sei noch erwähnt, dass hierin auch persönliche Daten der Angehörigen der WWU verwaltet werden. Diese Daten stammen aus dem Identitätsmanagement des hiesigen Rechenzentrums (ZIV) und werden mittels eines standardisierten [Netzwerkprotokolls (LDAP)](https://de.wikipedia.org/wiki/Lightweight_Directory_Access_Protocol) für die Nutzung der persönlichen ULB-Konten verwendet.   

2. (Retro)digitalisierung

Die ULB digitalisiert bereits seit einigen Jahren ihre historischen Bestände systematisch bzw. nach bestimmten strategischen Kriterien (Westfalica, Historische Drucke, Nachlässe) und verwendet hierfür die Software [Visual Library (VL)](https://www.semantics.de/visual_library/) der Aachener Fa. *semantics*. Die VL ist eine speziell für Bibliotheken und Informationseinrichtungen entwickelte Visualisierungssoftware u.a. im Bereich Retrodigitalisierung. In Verbindung mit der Software [Multidotscan](http://www.walternagel.de/multidotscan) der mit *semantics* kooperierenden Bielefelder Fa. *Walter Nagel* lässt sich der gesamte Prozess von der Digitalisierung bis hin zur Präsentation im Portal der ULB durchautomatisiert abbilden. Zunächst wird am jeweiligen Buchscanner ein Scan-Job eingerichtet, der für den Job eine eindeutige ID generiert (= sog. HT-Nummer) und per nächtlichem Import automatisch auf dem VL-Server abgelegt wird. Anschließend werden die Titeldaten über eine Abfrage der HT-Nummer aus dem Verbundkatalog des hbz in Köln importiert und gemeinsam mit den Images in der VL angezeigt. Nachdem anschließend die VL-Manager den Titel inhaltlich strukturiert haben (Inhaltsverzeichnis, Buchkapitel etc.), wird der Titel freigegeben, mit einer Nutzungslizenz (z.B. mit [Creative Commons Lizenzen](https://creativecommons.org/)) versehen und in den [Digitalen Sammlungen der ULB](https://sammlungen.ulb.uni-muenster.de/) angezeigt.  

3. Elektronischer Semesterapparat (ESA)

siehe Beispiel 1

4. Forschungsdatenmanagement

Ein weiteres Feld, in welchem in der ULB zahlreiche Daten fließen, ist der Bereich Forschungsdatenmanagement. Hier tritt die ULB gemeinsam mit dem zentralen Rechenzentrum (ZIV) als Servicedienstleister für die Wissenschaftler der WWU Münster auf, um die im Rahmen ihrer Forschungen auftretenden Daten aufzubereiten und langfristig zu speichern (max. 10 Jahre). In Form eines [Servicepunkts Forschungsdaten](https://www.uni-muenster.de/Forschungsdaten/angebote/ansprechpartner/) hilft die ULB den Wissenschaftlern bei forschungsdatenrelevanten Themen wie Antragstellung, Rahmenbedingungen, Datenmanagementplan, Datenschutz, Werkzeuge, Publizieren, Open Access und Langzeitarchivierung. Wenn man sich allein die im Forschungsdatenmanagement auftretenden Phasen als [Datenlebenszyklus](https://www.uni-muenster.de/Forschungsdaten/information/lebenszyklus/) vergegenwärtigt, wird klar, dass hier große Datenmengen und viele verschiedene Arten von Daten anfallen, und zwar bei der Erzeugung, Verarbeitung, Analyse, Archivierung, Zugang, Nachnutzung (und dann wieder) Erzeugung der Daten etc. Hierfür stellt die ULB gemeinsam mit dem ZIV die nötige IT-Infrastruktur und -software bereit und informiert auch im Rahmen von Schulungen und Workshops.

## Datenintensive Projekte mit ULB-Beteiligung

1. Fachinformationsdienst (FID) Benelux

Der Fachinformationsdienst (FID) Benelux ist ein DFG-gefördertes Projekt im Rahmen des Förderprogramms [Fachinformationsdienste für die Wissenschaft](https://www.dfg.de/foerderung/programme/infrastruktur/lis/lis_foerderangebote/fachinfodienste_wissenschaft/index.html) und ist das Nachfolgeformat der 2015 ausgelaufenen Sondersammelgebiete (SSG), hier des SSG Niederländischer Kulturkreis. Damit der FID seinem Selbstverständnis als „zentrale Anlaufstelle für forschungsrelevante Literatur und Informationen über die Kultur und Gesellschaft der Beneluxländer sowie für forschungsunterstützende Services“ gerecht werden kann, gibt es neben zahlreichen Services in der ULB das [Rechercheportal FID Benelux](https://www.fid-benelux.de/literatur-recherche/suche/) als zentrale Suchmaschine für forschungsrelevante Literatur über die Kultur und Gesellschaft der Beneluxländer. Um diese spezielle Literatur in einem eigenen Portal suchbar machen zu können, werden Benelux-relevante Titel von einschlägigen liefernden Bibliotheken entweder über MARC21XML-Exporte über einen FTP-Server in die zentrale Datenbank der ULB importiert, um von dort aus in den Online-Katalog und in das Discovery-System zu gelangen. Ein anderer Weg, um an relevante Literatur für das Rechercheportal zu gelangen, ist das Harvesten (Einsammeln) relevanter Literatur über offene [OAI-Schnittstellen](http://www.openarchives.org/), die den offenen Austausch von Metadaten zum Ziel haben. Hierfür kommen zumeist sog. „OAI-Harvester“ zum Einsatz, wie etwa auch in der [Deutschen Nationalbibliothek (DNB)](https://www.dnb.de/DE/Service/DigitaleDienste/OAI/oai_node.html). Um die eingesammelten Metadaten nun mit dem genannten FTP-Server und der zentralen Datenbank der ULB zu synchronisieren und semantisch verstehbar zu machen, werden spezielle Computerprogramm namens [Parser](https://de.wikipedia.org/wiki/Parser) eingesetzt. 

2. Digitalisierung Historischer Zeitungen NRW

siehe Beispiel 2

3. Digitales Archiv NRW (DA NRW)

Das Landesprojekt Digitales Archiv NRW hat sich auf ihrer [Webseite]((https://www.danrw.de/ueber-das-da-nrw/die-nrw-loesung/)) Folgendes  auf ihre Fahnen geschrieben: „Das DA NRW ist ein informationstechnisches Angebot für alle Einrichtungen, die ihr elektronisches Kulturgut nach dem Archivgesetz und Pflichtexemplargesetz sicher und auf Dauer speichern müssen. Zur Nutzung von Synergieeffekten wird die Dienstleistung in einem Verbund verschiedener Dienstleister des Landes und der Kommunen bereitgestellt.“ Zudem sollen die Daten nicht nur langzeitarchiviert bzw. -gespeichert werden, sondern auch für die Zulieferung in weitere Portal dienen ([DDB](https://www.deutsche-digitale-bibliothek.de/), [Europeana](https://www.europeana.eu/portal/de)). Als Testpartner der ersten Stunde hat die ULB Münster bereits Erfahrungen mit dem Workflow gesammelt, wie und auf welchen Wegen die Daten ins DA NRW migriert werden können. Für die Migration der Daten ins DA NRW spielt die Fa. *semantics* insofern eine wichtige Rolle, als sie für VL-Bibliotheken die vorhandenen Daten aus den jeweiligen Portalen in sog. [Submission Information Packages (SIP)](https://documents.clockss.org/index.php?title=Definition_of_SIP#OAIS_Submission_Information_Package_.28SIP.29) automatisiert zusammenfasst und per [rsync-Netwerkprotokollen](https://de.wikipedia.org/wiki/Rsync) sowie einer Authentifizierung per [SSH-keys](https://wiki.archlinux.org/index.php/SSH_keys) ins DA NRW hochgeladen werden können. Hierfür muss die VL in einem automatisierten Serverjob die besagten SIP-Kapseln erstellen und in das Incomingverzeichnis des hbz in Köln transferieren. Für später veränderte Objekte werden Delta-Kapseln erstellt, damit auch die Veränderungen an Objekten und ihren Metadaten erfasst werden. Die Verwaltung der SIP-Kapseln selbst erfolgt im VL-Manager. Hier können die MitarbeiterInnen der jeweiligen Einrichtung verfolgen, ob eine SIP-Kapsel erfolgreich erstellt, übertragen und archiviert worden ist. Außerdem gibt es im VL-Manager Such- und Filtermöglichkeiten (VL-ID, [URN](https://de.wikipedia.org/wiki/Uniform_Resource_Name), HBZ-ID) sowie eine Archivierungshistorie. Die Datenübertragungsstraßen von der VL ins DA NRW werden zurzeit eingerichtet und voraussichtlich im Sommer 2019 zur Nutzung freigegeben.

4. Linked Open Data University of Münster (LODUM)

[LODUM](https://www.uni-muenster.de/LODUM/) ist die Open Data Initiative an der WWU Münster. Sie hat sich zum Ziel gesetzt, alle öffentlichen Information über die Universität in maschinenlesbaren Formaten für den einfachen Zugang und zur einfachen Wiederverwendung zur Verfügung zu stellen. LODUM möchte einen One-Stop Shop für alle Daten der WWU voranbringen, indem Daten von unterschiedlichen Informationssystemen geöffnet und kreuzreferenziert werden. Schon jetzt können die [ULB-Daten](https://www.uni-muenster.de/LODUM/data/ulb/) des Online-Kataloges, der Digitalen Sammlungen, der Lynda-Video-Trainings, des Open Journal Systems (OJS), historischer Musiknoten und des Publikationsservers miami tagesaktuell als [zip](https://de.wikipedia.org/wiki/ZIP-Dateiformat)-Exporte heruntergeladen werden. Auch wenn bislang noch recht wenige Institutionen der WWU ihre Daten über den LODUM-Server bereit stellen, bildet die Initiative einen wichtigen Baustein in der Open Data Strategie der WWU Münster.

5. Opening Reproducible Research (O2R)

Das Projekt [Opening Reproducible Research (O2R)](https://o2r.info/) ist ein DFG-gefördertes Projekt vom Münsteraner Institut für Geoinformatik und der ULB Münster. Es lief in einer ersten Phase von 2015 bis 2018 und hat erste Strukturen geschaffen, um die in Publikationen entstandenen Forschungsergebnisse reproduzierbar zu machen. Auf der [Projekt-Webseite](https://www.uni-muenster.de/Geoinformatics/research/projects/O2R.html) heißt es: Mit dem Projekt „zielen wir direkt auf zentrale Aspekte von Open Access ab, indem wir den Austausch von online veröffentlichten Forschungsergebnissen verbessern, die produktive Aneignung ermöglichen und deren Wiederverwendung vereinfachen.“ Im April 2019 hat die DFG hat das Folgeprojekt „O2R2“ für weitere 30 Monate bewilligt. In dieser zweiten Phase soll es nun darum gehen, die entwickelten Protoypen aus der ersten Phase mit existenten Artikeln zu testen und auf einer gemeinsam entwickelten Plattform die Forschungsergebnisse dieser Artikel reproduzierbar zu machen.

6. sciebo-research Data Services (sciebo-RDS)

Ebenfalls in den Bereich Forschungsdatenmanagement zielt das auf drei Jahre von der DFG geförderte Projekt „sciebo-research Data Services (sciebo-RDS) – Forschungsdatenmanagementdienste und -werkzeuge für Wissenschaftler“. Es handelt sich um ein [Kooperationsprojekt](https://www.forschungsdaten.info/praxis-kompakt/fdm-in-den-bundeslaendern/nordrhein-westfalen/projekte/sciebords/) der ULB Münster, dem Rechenzentrum (ZIV) der WWU Münster und der Abteilung Informatik und Angewandte Kognitionswissenschaft der Universität Duisburg Essen. Als Kooperationspartner konnte die Universität Bielefeld gewonnen werden. In diesem Projekt geht es darum, ein niederschwelliges Angebot für das Management von Forschungsdaten zu entwickeln. Auf der Basis des Cloudspeicherdienstes sciebo, den das ZIV bereitstellt, sollen Werkzeuge, Workflows und Services entwickelt werden, um den Forschenden bei der Durchführung eines strukturierten Forschungsdatenmanagements zu helfen. 

## Beispiel: Digitalisierung Historischer Zeitungen NRW

Im Rahmen eines vom Land Nordrhein-Westfalen zunächst für drei Jahre geförderten Projekts zur „Digitalisierung von historischen Zeitungen in Nordrhein-Westfalen“ wird gemeinsam mit der ULB Bonn ein repräsentativer Querschnitt der historischen nordrhein-westfälischen Zeitungen aus dem Erscheinungszeitraum 1801 bis 1945 digitalisiert und sowohl für die Forschung, als auch für die interessierte Öffentlichkeit kostenfrei und komfortabel im Zeitungsportal [zeit.punktNRW](https://zeitpunkt.nrw/) zur Verfügung gestellt. Auf dieser Plattform, die vom hbz in Köln gehostet wird, werden differenzierte Such- und Präsentationsmöglichkeiten geschaffen, die optimal auf die speziellen Objekte sowie auf die Fragestellungen verschiedener Nutzergruppen zugeschnitten sind.

Da sich das Projekt als Massendigitalisierungsprojekt versteht, werden historische Zeitungen in der ersten (und auch in einer möglichen zweiten Projektphase ab 2020) vornehmlich von Mikrofilmen digitalisiert. Hierfür wurde ein bestimmter Workflow entwickelt, der im Folgenden dargestellt und mit einer Idde zur Optimierung des Workflows endet:

Im ersten Schritt werden die Mikrofilme mittels eines Mikrofilmscanners der *[Fa. Zeutschel OM 1600]* digitalisiert und damit Rohdaten für die weitere Verarbeitung erstellt. Die Digitalisate werden zunächst auf der lokalen Festplatte gespeichert und anschließend weiter bearbeitet. Dies ist notwendig, da die Zeitungsseiten in Form von sog. Strips noch unbeschnitten und entsprechend nicht zu gebrauchen sind. Die Bearbeitung dieser Rohdaten erfolgt mittels der Software [Quantum Process (Scan Version 1.02.26)](http://www.acmisgroup.com/en/scanners/detail/mekel-quantum-process-software) der [*Fa. acmisgroup*](http://www.acmisgroup.com/en). Über ein lokales Netzlaufwerk werden die Rohdaten nun in den Quantum Process geladen und dort bearbeitet. Zur Bearbeitung gehört die korrekte Rahmensetzung (Frames), ggf. die Drehung und Trennung von vorhandenen Doppelseiten. Das Ergebnis der Bearbeitung wird in einer [.qpf-Datei](https://www.reviversoft.com/de/file-extensions/qpf) gespeichert, mit der die Einstellung und die Parameter zu einem späteren Zeitpunkt wieder aufgerufen werden können. Anschließend werden mit Hilfe dieser qpf-Dateien die sog. Outputdateien im [TIFF-Format](https://de.wikipedia.org/wiki/Tagged_Image_File_Format) erstellt. Diese TIFF-Dateien werden auf dem lokalen Server mittels einer Batch-Datei als ZIP-Datei verpackt:

https://user-images.githubusercontent.com/49555636/57213930-8e2e3700-6fe8-11e9-9e41-3392864e5749.jpg

Wichtig hierbei ist, dass die Batch-Datei zip.bat zusammen mit den Outputdatei-Verzeichnissen liegen müssen

https://user-images.githubusercontent.com/49555636/57214056-0268da80-6fe9-11e9-9e17-0f28862fc152.png

Alle ZIP-Dateien werden nun über einen [SFTP](https://de.wikipedia.org/wiki/SSH_File_Transfer_Protocol)-Server manuell mittels Log-Dateien und dem Programm FileZilla in das Upload-Verzeichnis „ULB MS“ des hbz hochgeladen. Hier wird eine Verbindung zu zdiginrw.hbz.nrw.de aufgebaut, anschließend in das Verzeichnis zdiginrwulbms gewechselt und die Dateien hochgeladen:

https://user-images.githubusercontent.com/49555636/57214423-3abce880-6fea-11e9-9ca2-057c11ae3f6d.png

In einem nächtlichen stattfindenden VL-Batchprozess werden die Daten automatisch entpackt und in die VL importiert:

https://user-images.githubusercontent.com/49555636/57214432-3f819c80-6fea-11e9-90c7-762af6a5ef50.png

Am folgenden Tag kann dann die Bearbeitung der hochgeladenen Images durch die VL-Manager erfolgen:

https://user-images.githubusercontent.com/49555636/57214437-427c8d00-6fea-11e9-8f0f-e341eeb26029.png

**Optimierungspotenzial im Workflow**

Im drittletzten Schritt des beschriebenen Datenfluss-Workflows könnte man aus Sicht des Autors den Prozess optimieren, indem man den bislang manuell durchgeführten Vorgang, die ZIP-Dateien manuell in das Upload-Verzeichnis hochzuladen, automatisiert. Könnte man hierfür (Frage an den Dozenten) ebenfalls eine Batch-Datei schreiben, die genau diesen Vorgang initiiert, sobald ZIP-Dateien auf dem lokalen Server generiert worden sind? Außerdem müsste in diesem Skript stehen, dass Dateien nur bis zu einer vorher definierten Größe in den Upload-Server hochgeladen werden, um diesen nicht "vollaufen" zu lassen. Dies ist im Übrigen auch der Hauptgrund, der mir genannt wurde, wieso dieser Teilprozess bislang manuell abläuft. Wenn man allerdings diesen Aspekt mitberücksichtigt, dürfte diese Gefahr beim Upload auszuschließen sein.

